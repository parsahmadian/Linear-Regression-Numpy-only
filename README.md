# Linear Regression from Scratch (Python & NumPy)

This project is a **from-scratch implementation of multivariate linear regression**
using only **Python and NumPy**, without any machine learning libraries.

## ğŸ¯ Goal
To deeply understand:
- How linear regression works internally
- How loss functions affect learning
- How gradient descent updates model parameters
- The impact of learning rate and feature scale on training stability

## ğŸ§  Model
The model predicts output using:
```
yÌ‚ = X Â· w + b
```

Where:
- X: input features
- w: weights
- b: bias

Loss function:
- Mean Squared Error (MSE)

Optimization:
- Gradient Descent

## ğŸ“Š Results
- Training loss decreases steadily
- Demonstrates convergence behavior
- Shows instability when learning rate is too large (overflow / NaN)

## âš ï¸ Notes
- In previous versions, No feature standardization was applied intentionally to observe its effect
- Learning rate was carefully tuned to avoid divergence

## ğŸ›  Tech Stack
- Python
- NumPy
- Pandas
- Matplotlib

## ğŸš€ Why this project?
This project focuses on **conceptual understanding**, not just usage of libraries.
It demonstrates my ability to:
- Implement ML algorithms from scratch
- Debug numerical instability
- Reason about optimization behavior
